library(quadprog)
library(ROI)
library(ROI.plugin.glpk)
library(ROI.plugin.quadprog)
library(ROI.plugin.symphony)
library(rJava)
library(xlsx)
library(readxl)
bondR <- read.csv(file = "D:/Document/CMCRCTraining/Fortlake/CreditRisk/Model2/bondRannual.csv", header = TRUE)
bondR$date <- as.Date(as.character(bondR$date), "%d-%h-%y")
bondR <- xts(bondR[,-1], bondR[, 1])
which(is.na(bondR), arr.ind=TRUE)
bondRmatrix <- as.matrix(bondR)
bondinfo <- read.csv(file = "D:/Document/CMCRCTraining/Fortlake/CreditRisk/Model2/bondInfo.csv", header = TRUE)
bondlist <- names(bondR)
bondYTM <- c()
for (b in bondlist){
bondYTM <- c(bondYTM,bondinfo[bondinfo$ID_BB_UNIQUE==b,]$Mid.YTM)
}
bondRating <- c()
for (b in bondlist){
bondRating <- c(bondRating,bondinfo[bondinfo$ID_BB_UNIQUE==b,]$Numeric.Rating)
}
#specify the portfolio object
pspec <- portfolio.spec(assets = bondlist)
# pspec <- portfolio.spec(assets = bondlist)
print.default(pspec)
pspec$constraints
#add a sum of weights constraint
pspec <- add.constraint(portfolio = pspec , type = "weight_sum", min_sum = 0.99, max_sum =1.01)
pspec <- add.constraint(portfolio=pspec, type="position_limit", max_pos=30)
#pspec <- add.constraint(portfolio=pspec, type="full_investment") #special case of min_sum=max_sum=1
#pspec <- add.constraint(portfolio=pspec, type="long_only")
#add a box constraint to specify min and max holdings
pspec <- add.constraint(portfolio = pspec, type = "box", min = 0, max = 0.1)
#Add a rating constraint
pspec <- add.constraint(portfolio = pspec , type = "factor_exposure", B = bondRating, lower = 65, upper = 100)
#Add a ytm constraint
pspec <- add.constraint(portfolio = pspec , type = "factor_exposure", B = bondYTM, lower = 1.5, upper = 5)
summary(pspec)
#add otmp1.mean <- apply(rp1, 1, function(x) mean(R %*% x))
#objective function
#pspec <- add.constraint(portfolio= pspec, type="full_investment")
#pspec <- add.constraint(portfolio= pspec, type="long_only")
#pspec <- add.objective(portfolio = pspec, type = "return", name = "return")
pspecmaxSharpe <- add.objective(portfolio = pspec, type="risk", name="StdDev")
expret = colMeans(x=bondRmatrix, na.rm = TRUE)
#
# library(foreach)
# library(doParallel)
# #setup parallel backend to use many processors
# cores=detectCores()
# cl <- makeCluster(cores[1]-1) #not to overload your computer
# registerDoParallel(cl)
ptm <- proc.time()
pspec <- add.objective(portfolio = pspecmaxSharpe, type="return", name="mean")
maxSharpe_rp <- optimize.portfolio(R=bondRmatrix, portfolio=pspec,
optimize_method= c("DEoptim",
"random", "ROI", "pso", "GenSA")[2],
search_size=1000000, maxSR=TRUE,
trace=TRUE)
proc.time()-ptm
out_maxsharpe <- maxSharpe_rp$weights
length(out_maxsharpe)
length(which(out_maxsharpe==0))
sum(out_maxsharpe)
out_maxsharpe%*%bondRating
out_maxsharpe%*%bondYTM
result <-maxSharpe_rp[["objective_measures"]]
stdet <- result$StdDev[1,1]
meanret <- result$mean
meanret/stdet
# stopCluster(cl)
weights2 <- out_maxsharpe[bondinfo$ID_BB_UNIQUE]
w <- data.frame(ID_BB_UNIQUE=bondinfo$ID_BB_UNIQUE,
PARSEKEYABLE_DES=bondinfo$PARSEKEYABLE_DES,
ISIN=bondinfo$ID_ISIN,
Ticker= bondinfo$Ticker,
Coupon= bondinfo$Coupon,
Maturity.Date= bondinfo$Maturity.Date,
Sector= bondinfo$Sector,
Mid.YTM= bondinfo$Mid.YTM,
Rating.S.P= bondinfo$Rating.S.P,
Rating.Moody= bondinfo$Rating.Moody,
Fitch.Rating= bondinfo$Fitch.Rating,
YAS_MOD_DUR= bondinfo$YAS_MOD_DUR,
weights2=weights2)
write.table(w,"D:/Document/CMCRCTraining/Fortlake/CreditRisk/Model2/bondweights_newmethod1.csv", sep=",", row.names = FALSE,
col.names = c("ID_BB_UNIQUE","PARSEKEYABLE_DES","ISIN","Ticker","Coupon","Maturity.Date",
"Sector","Mid.YTM","Rating.S.P","Rating.Moody","Fitch.Rating","YAS_MOD_DUR","Weight"))
library(parma, quietly = TRUE, verbose = FALSE)
library(quantmod)
library(parallel)
library("writexl")
library(PortfolioAnalytics)
library(DEoptim)
library(Rglpk)
library(Rsymphony)
library(quadprog)
library(ROI)
library(ROI.plugin.glpk)
library(ROI.plugin.quadprog)
library(ROI.plugin.symphony)
library(rJava)
library(xlsx)
library(readxl)
bondR <- read.csv(file = "D:/Document/CMCRCTraining/Fortlake/CreditRisk/Model2/bondRannual.csv", header = TRUE)
bondR$date <- as.Date(as.character(bondR$date), "%d-%h-%y")
bondR <- xts(bondR[,-1], bondR[, 1])
which(is.na(bondR), arr.ind=TRUE)
bondRmatrix <- as.matrix(bondR)
bondinfo <- read.csv(file = "D:/Document/CMCRCTraining/Fortlake/CreditRisk/Model2/bondInfo.csv", header = TRUE)
bondlist <- names(bondR)
bondYTM <- c()
for (b in bondlist){
bondYTM <- c(bondYTM,bondinfo[bondinfo$ID_BB_UNIQUE==b,]$Mid.YTM)
}
bondRating <- c()
for (b in bondlist){
bondRating <- c(bondRating,bondinfo[bondinfo$ID_BB_UNIQUE==b,]$Numeric.Rating)
}
#specify the portfolio object
pspec <- portfolio.spec(assets = bondlist)
# pspec <- portfolio.spec(assets = bondlist)
print.default(pspec)
pspec$constraints
#add a sum of weights constraint
pspec <- add.constraint(portfolio = pspec , type = "weight_sum", min_sum = 0.99, max_sum =1.01)
pspec <- add.constraint(portfolio=pspec, type="position_limit", max_pos=30)
#pspec <- add.constraint(portfolio=pspec, type="full_investment") #special case of min_sum=max_sum=1
#pspec <- add.constraint(portfolio=pspec, type="long_only")
#add a box constraint to specify min and max holdings
pspec <- add.constraint(portfolio = pspec, type = "box", min = 0, max = 0.1)
#Add a rating constraint
pspec <- add.constraint(portfolio = pspec , type = "factor_exposure", B = bondRating, lower = 65, upper = 100)
#Add a ytm constraint
pspec <- add.constraint(portfolio = pspec , type = "factor_exposure", B = bondYTM, lower = 1.5, upper = 5)
summary(pspec)
#add otmp1.mean <- apply(rp1, 1, function(x) mean(R %*% x))
#objective function
#pspec <- add.constraint(portfolio= pspec, type="full_investment")
#pspec <- add.constraint(portfolio= pspec, type="long_only")
#pspec <- add.objective(portfolio = pspec, type = "return", name = "return")
pspecmaxSharpe <- add.objective(portfolio = pspec, type="risk", name="StdDev")
expret = colMeans(x=bondRmatrix, na.rm = TRUE)
#
# library(foreach)
# library(doParallel)
# #setup parallel backend to use many processors
# cores=detectCores()
# cl <- makeCluster(cores[1]-1) #not to overload your computer
# registerDoParallel(cl)
ptm <- proc.time()
pspec <- add.objective(portfolio = pspecmaxSharpe, type="return", name="mean")
maxSharpe_rp <- optimize.portfolio(R=bondRmatrix, portfolio=pspec,
optimize_method= c("DEoptim",
"random", "ROI", "pso", "GenSA")[1],
search_size=5000, maxSR=TRUE,
trace=TRUE)
proc.time()-ptm
library(neuralnet)
AND <- c(rep(0,7),1)
OR <- c(0,rep(1,7))
binary.data <- data.frame(expand.grid(c(0,1), c(0,1), c(0,1)), AND, OR)
set.seed(3)
print(net <- neuralnet(AND+OR~Var1+Var2+Var3,  binary.data, hidden=0, rep=10, err.fct="sse", linear.output=FALSE))
#Call: neuralnet(formula = AND + OR ~ Var1 + Var2 + Var3, data = binary.data,     hidden = 0, rep = 10, err.fct = "sse", linear.output = FALSE)
#
#10 repetitions were calculated.
#
#Error Reached Threshold Steps
#7  0.04043122185    0.008248439644   116
#5  0.04426319054    0.009619409680   124
#8  0.04698485282    0.007947430014   117
#2  0.04931335384    0.008792873261    88
#1  0.04965332555    0.009631079320    89
#4  0.05396400022    0.009092193542    96
#6  0.05488395412    0.009990028287   124
#3  0.06383087672    0.009964206587    94
#10 0.51657348285    0.008602371325    51
#9  0.52514202592    0.007890927099    40
set.seed(3)
custom <- function(x,y){abs(y-x)}
net <- neuralnet(AND+OR~Var1+Var2+Var3,  binary.data, hidden=0, rep=10,  err.fct=custom)
softplus <- function(x) log(1 + exp(x))
net <- neuralnet(AND+OR~Var1+Var2+Var3,  binary.data, hidden=0, rep=10,  err.fct=custom,act.fct = softplus,linear.output=FALSE)
custom <- function(x,y){1/2*(y-x)^2}
net <- neuralnet(AND+OR~Var1+Var2+Var3,  binary.data, hidden=0, rep=10,  err.fct=custom,act.fct = softplus,linear.output=FALSE)
set.seed(3)
custom <- function(x,y){1/2*(y-x)^2}
print(net <- neuralnet(AND+OR~Var1+Var2+Var3,  binary.data, hidden=0, rep=10, linear.output=FALSE, err.fct=custom))
gc()
rm(list = ls())
.rs.restartR()
# load the data set
library(keras)
install_keras()
?install_keras
install.packages('Rcpp')
install.packages("Rcpp")
# load the data set
library(keras)
install_keras()
# load the data set
library(keras)
install_keras()
install.packages('Rcpp')
install.packages("Rcpp")
install_keras()
# load the data set
library(keras)
install_keras()
# load the data set
library(keras)
install_keras()
library(keras)
install_keras()
# load the data set
library(keras)
install.packages("keras")
install_keras()
# load the data set
library(keras)
install_keras()
Y
# The model as specified in "Deep Learning with R"
model <- keras_model_sequential() %>%
input_shape = c(64) %>%
layer_dense(units = 1)\\
model <- keras_model_sequential() %>%
layer_dense(units = 64, activation = "relu",
input_shape = c(64) %>%
layer_dense(units = 64, activation = "relu") %>%
layer_dense(units = 1)\\
# load the data set
library(keras)
# The model as specified in "Deep Learning with R"
model <- keras_model_sequential() %>%
input_shape = c(64) %>%
layer_dense(units = 1)\\
library(neuralnet)
model <- keras_model_sequential() %>%
layer_dense(units = 64, activation = "relu",
input_shape = 64) %>%
layer_dense(units = 64, activation = "relu") %>%
layer_dense(units = 1)
install_tensorflow()
# load the data set
library(keras)
install_tensorflow()
library(tensorflow)
install_tensorflow()
# load the data set
library(keras)
model <- keras_model_sequential() %>%
layer_dense(units = 64, activation = "relu",
input_shape = 64) %>%
layer_dense(units = 64, activation = "relu") %>%
layer_dense(units = 1)
?install_keras
virtualenv python
reticulate::py_config()
reticulate::py_discover_config()
extra <- "C:/Users/Hang/Anaconda3;C:/Users/Hang/Anaconda3/Library/mingw-w64/bin;C:/Users/Hang/Anaconda3/Library/usr/bin;C:/Users/Hang/Anaconda3/Library/bin;C:/Users/Hang/Anaconda3/Scripts;C:/Users/Hang/Anaconda3/bin;C:/Users/Hang/Anaconda3/condabin;"
Sys.setenv(PATH = paste(extra, Sys.getenv("PATH"), sep = ""))
#Load Packages
install.packages("BiocManager")
library("BiocManager")
BiocManager::install("EBImage")
reticulate::py_config()
reticulate::py_discover_config()
sudo R -e "remove.packages('BiocManager')"
Rscript.exe -e -e "remove.packages('BiocManager')"
Rscript.exe -e "remove.packages('BiocManager')"
extra <- "C:/Users/Hang/Anaconda3;C:/Users/Hang/Anaconda3/Library/mingw-w64/bin;C:/Users/Hang/Anaconda3/Library/usr/bin;C:/Users/Hang/Anaconda3/Library/bin;C:/Users/Hang/Anaconda3/Scripts;C:/Users/Hang/Anaconda3/bin;C:/Users/Hang/Anaconda3/condabin;"
Sys.setenv(PATH = paste(extra, Sys.getenv("PATH"), sep = ""))
Rscript.exe -e "remove.packages('BiocManager')"
#Load Packages
install.packages("BiocManager")
library("BiocManager")
BiocManager::install("EBImage")
library(EBImage)
library(keras)
install_keras()
library(tensorflow)
install_tensorflow()
reticulate::py_config()
reticulate::py_discover_config()
library(tensorflow)
tf_config()
devtools::install_github("rstudio/tensorflow")
install.packages("devtools")
devtools::install_github("rstudio/tensorflow")
devtools::install_github("rstudio/tensorflow")
writeLines('PATH="${RTOOLS40_HOME}\\usr\\bin;${PATH}"', con = "~/.Renviron")
Sys.which("make")
versionInfo()
rstudioapi::versionInfo()
install.packages("rstudioapi")
versionInfo()
rstudioapi::versionInfo()
install_keras()
library(keras)
install_keras()
install.packages("keras")
install_keras()
library(keras)
install_keras()
extra <- "C:/Users/Hang/Anaconda3;C:/Users/Hang/Anaconda3/Library/mingw-w64/bin;C:/Users/Hang/Anaconda3/Library/usr/bin;C:/Users/Hang/Anaconda3/Library/bin;C:/Users/Hang/Anaconda3/Scripts;C:/Users/Hang/Anaconda3/bin;C:/Users/Hang/Anaconda3/condabin;"
Sys.setenv(PATH = paste(extra, Sys.getenv("PATH"), sep = ""))
#Load Packages
install.packages("BiocManager")
library("BiocManager")
BiocManager::install("EBImage")
library(EBImage)
library(EBImage)
library(keras)
install_keras()
BiocManager::install("EBImage")
library(EBImage)
install.packages("RCurl")
library(EBImage)
BiocManager::install("EBImage")
BiocManager::install("EBImage",force=TRUE)
devtools::install_github("rstudio/tensorflow")
install.packages("devtools")
devtools::install_github("rstudio/tensorflow")
tensorflow::install_tensorflow()
tensorflow::install_tensorflow()
library(keras)
install_keras()
library(tensorflow)
install_tensorflow()
model <- keras_model_sequential() %>%
layer_dense(units = 64, activation = "relu",
input_shape = 64) %>%
layer_dense(units = 64, activation = "relu") %>%
layer_dense(units = 1)
install_keras(method = "conda")
# load the data set
library(keras)
install_keras(method = "conda")
data <- dataset_boston_housing()
c(c(train_data,train_targets), c(test_data,test_targets)) %<-% data
# transform the training and test labels
train_targets <- (train_targets*1000)^2/2500000
test_targets <- (test_targets*1000)^2/2500000
train_targets
# The model as specified in "Deep Learning with R"
model <- keras_model_sequential() %>%
layer_dense(units = 64, activation = "relu",
input_shape = dim(train_data)[[2]]) %>%
layer_dense(units = 64, activation = "relu") %>%
layer_dense(units = 1)
losses <- c(keras::loss_mean_squared_error,
keras::loss_mean_squared_logarithmic_error, MLAE, MSLAE)
model %>% compile(
optimizer = "rmsprop",
loss = losses[1],
metrics = c("mae")
)
losses <- c(keras::loss_mean_squared_error,
keras::loss_mean_squared_logarithmic_error)
model %>% compile(
optimizer = "rmsprop",
loss = losses[1],
metrics = c("mae")
)
model %>% fit(
train_data,
train_targets,
epochs = 100,
batch_size = 5,
verbose = 1,
validation_split = 0.2
)
library(keras)
data <- dataset_boston_housing()
c(c(train_data,train_targets), c(test_data,test_targets)) %<-% data
# transform the training and test labels
train_targets <- (train_targets*1000)^2/2500000
test_targets <- (test_targets*1000)^2/2500000
model <- keras_model_sequential() %>%
layer_dense(units = 64, activation = "relu",
input_shape = dim(train_data)[[2]]) %>%
layer_dense(units = 64, activation = "relu") %>%
layer_dense(units = 1)
losses <- c(keras::loss_mean_squared_error,
keras::loss_mean_squared_logarithmic_error)
model %>% compile(
optimizer = "rmsprop",
loss = losses[1],
metrics = c("mae")
)
# Train the model with validation
model %>% fit(
train_data,
train_targets,
epochs = 100,
batch_size = 5,
verbose = 1,
validation_split = 0.2
)
# Calculate the mean absolute error
results <- model %>% evaluate(test_data, test_targets, verbose = 0)
results$mean_absolute_error
results
library(keras)
numvar1 <- 100
numvar2 <- 30
numnode_conv <- 100
conv_model <- keras_model_sequential()
conv_model %>%
layer_conv_1d(filters = 32, kernel_size = 3, activation = 'relu',
input_shape = c(numvar1, 1)) %>%
layer_conv_1d(filters = 32, kernel_size = 3, activation = 'relu') %>%
layer_average_pooling_1d(pool_size = 3)%>%
layer_flatten() %>%
layer_dense(units = numnode_conv, activation = "relu")
# layer_conv_1d(filters = 128, kernel_size = 3, activation = 'relu') %>%
# layer_conv_1d(filters = 128, kernel_size = 3, activation = 'relu') %>%
# layer_global_average_pooling_1d() %>%
fc_model <- keras_model_sequential()
fc_model %>%
layer_dense(units = numnode_conv+numvar2, activation = "relu", input_shape = numnode_conv+numvar2) %>%
layer_dense(units = numnode_conv+numvar2, activation = "relu") %>%
layer_dense(units = 1)
input1 <- layer_input(c(numvar1))
input2 <- layer_input(c(numvar2))
output1 <- input1 %>% conv_model
inputconv <- layer_concatenate(list(output1, input2))
output <- inputconv%>%fc_model
model <-  keras_model( list(input1 , input2) , output )
early_stop <- callback_early_stopping(monitor = "val_loss",
min_delta = 0.1,
patience = 2,
restore_best_weights = TRUE,
verbose = 0)
losses <- c(keras::loss_mean_absolute_percentage_error,
keras::loss_mean_absolute_error,
keras::loss_mean_squared_error,
keras::loss_mean_squared_logarithmic_error)
model %>% compile(
optimizer = "rmsprop",
loss = losses[3],
metrics = c("mse")
)
trainx1 <- matrix( rnorm(100*numvar1,mean=0,sd=1), 100, numvar1)
trainx2 <- matrix( rnorm(100*numvar2,mean=0,sd=1), 100, numvar2)
trainy <- rnorm(100,mean=0,sd=1)
model %>% fit(
list(trainx1,trainx2),
trainy,
epochs = 500,
batch_size = 128,
verbose = 1,
validation_split = 0.2,
callbacks = list(
early_stop)
)
trainy <- rnorm(100,mean=0,sd=1)%>% as.matrix()
dim(trainy)
model %>% fit(
list(trainx1,trainx2),
trainy,
epochs = 500,
batch_size = 128,
verbose = 1,
validation_split = 0.2,
callbacks = list(
early_stop)
)
install.packages("reticulate")
install.packages("reticulate")
install.packages("reticulate")
install.packages("reticulate")
install.packages("reticulate")
install.packages("reticulate")
#Change path to regPolfull in the code
setwd("D:/Document/Research/eclipse-workspace-nested/VariableAnnuitiesparallel")
library(dbscan)
library(funHDDC)
library(neuralnet)
library(nnet)
library(dplyr)
library(plyr)
library(keras)
library(tidyverse)
library(kerastuneR)
library(plotly)
library(clhs)
library(readr)
library(data.table)
#kerastuneR::install_kerastuner(python_path = 'C:/Users/hangn/Anaconda3')
#memory.limit(size=1800000)
normalize <- function(x) {
if (max(x) == min(x)){
return (x)
}
return ((x - min(x)) / (max(x) - min(x)))
}
scenariofolder <-  "src/Result/inforcevaluation/RW10000/"
file_FixedIncome <-  paste(scenariofolder ,"base/FixedIncome.csv", sep = "")
file_IntlEquity <-  paste(scenariofolder, "base/IntlEquity.csv", sep = "")
file_LargeCapEquity <-  paste(scenariofolder, "base/LargeCapEquity.csv", sep = "")
file_SmallCapEquity <-  paste(scenariofolder, "base/SmallCapEquity.csv", sep = "")
file_Money <-  paste(scenariofolder, "base/Money.csv", sep = "")
fixedIncome <- read.csv(file = file_FixedIncome, header = FALSE)
fixedIncome <- fixedIncome[,-1]
intlEquity <- read.csv(file = file_IntlEquity, header = FALSE)
intlEquity <- intlEquity[,-1]
largeCapEquity <- read.csv(file = file_LargeCapEquity, header = FALSE)
